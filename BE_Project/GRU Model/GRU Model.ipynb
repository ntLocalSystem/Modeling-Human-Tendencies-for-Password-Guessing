{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Embedding, LSTM, Reshape, GRU\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import math\n",
    "import errno\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "# Scheduling a learning-rate to produce different effects for gradient of loss wrt to the weights\n",
    "# Different Learning Rates will affect the model differently by updating different % of weights of the model.\n",
    "START_CHAR = \"\\t\"\n",
    "END_CHAR = \"\\n\"\n",
    "GRU_UNITS = 300\n",
    "DENSE_UNITS = 100\n",
    "VOCAB_SIZE = 70\n",
    "PROB_THRESHOLD = 1e-9\n",
    "VERBOSITY = 1\n",
    "EPOCHS = 15\n",
    "MAX_LENGTH = 32\n",
    "EMBEDDING_DIMENSION = 16\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 5000\n",
    "EMBEDDING_METADATA = 'metadata.tsv'\n",
    "PREFETCH_BATCHES = 10\n",
    "METRICS = [tf.keras.metrics.SparseCategoricalCrossentropy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the inputs and the outputs for the model --- \n",
    "# For Example password 'passwd'\n",
    "# Adding a \\t at the Start for the input - \\tpasswd \n",
    "# Adding a \\n at the End for the model output - passwd\\n\n",
    "# Here \\t predicts p, p predicts a and so on..\n",
    "def loadPreprocessInputOutput(FILE_NAME, START_CHAR, END_CHAR):\n",
    "    try:\n",
    "        assert os.path.isfile(FILE_NAME)\n",
    "        assert isinstance(START_CHAR, str)\n",
    "        assert isinstance(END_CHAR, str)\n",
    "    except:\n",
    "        raise Exception(\"Incorrect Inputs. Try again.\")\n",
    "        return\n",
    "    inputPasswords = []\n",
    "    outputPasswords = []\n",
    "    listPasswords = []\n",
    "    with open(FILE_NAME, \"r\") as pass_file:\n",
    "        while(True):\n",
    "            single_pass = pass_file.readline().rstrip(\"\\n\")\n",
    "            if(single_pass == \"\"):\n",
    "                break\n",
    "            else:\n",
    "                inputPasswords.append(START_CHAR + single_pass)\n",
    "                outputPasswords.append(single_pass + END_CHAR)\n",
    "                listPasswords.append(START_CHAR + single_pass + END_CHAR)\n",
    "        return(inputPasswords, outputPasswords, listPasswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputPasswords, outPasswords, listPasswords = loadPreprocessInputOutput(\"..\\\\Embedding\\\\Data\\\\ascii_rockyou_less_than_thirty_two_cleaned.txt\", START_CHAR, END_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\tprincess', '\\t1234567', '\\trockyou', '\\t12345678', '\\tabc123']\n",
      "14105697\n",
      "['princess\\n', '1234567\\n', 'rockyou\\n', '12345678\\n', 'abc123\\n']\n",
      "14105697\n",
      "['\\tprincess\\n', '\\t1234567\\n', '\\trockyou\\n', '\\t12345678\\n', '\\tabc123\\n']\n",
      "14105697\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print(inputPasswords[5:10])\n",
    "print(len(inputPasswords))\n",
    "print(outPasswords[5:10])\n",
    "print(len(outPasswords))\n",
    "print(listPasswords[5:10])\n",
    "print(len(listPasswords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the passwords:\n",
    "# Do not open file - will not be shown properly\n",
    "def writePreprocessedPasswordFromList(passwordList, typeList):\n",
    "    passwordFileName = typeList + \"_preprocessed.txt\"\n",
    "    with open(passwordFileName, \"w\") as pass_file:\n",
    "        for password in passwordList:\n",
    "            if(typeList.lower() == \"input\"):\n",
    "                pass_file.write(password+\"\\n\")\n",
    "            else:\n",
    "                pass_file.write(password)\n",
    "    print(f\"{typeList} passwords written completely to : {passwordFileName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the passwords\n",
    "writePreprocessedPasswordFromList(inputPasswords, \"input\")\n",
    "writePreprocessedPasswordFromList(outPasswords, \"output\")\n",
    "writePreprocessedPasswordFromList(listPasswords, \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t123456\n",
      "\t12345\n",
      "\t123456789\n",
      "\tpassword\n",
      "\tiloveyou\n",
      "123456\n",
      "12345\n",
      "123456789\n",
      "password\n",
      "iloveyou\n",
      "\t123456\n",
      "\t12345\n",
      "\t123456789\n",
      "\tpassword\n",
      "\tiloveyou\n",
      "14105697 input_preprocessed.txt\n",
      "14105697 output_preprocessed.txt\n",
      "14105697 list_preprocessed.txt\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "# Output will not be proper.\n",
    "!head -n 5 input_preprocessed.txt\n",
    "!head -n 5 output_preprocessed.txt\n",
    "!head -n 5 list_preprocessed.txt\n",
    "!wc -l input_preprocessed.txt\n",
    "!wc -l output_preprocessed.txt\n",
    "!wc -l list_preprocessed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specifying the Tokenizer -- leaving num_words as blank to include as many \n",
    "# unique characters as possible.\n",
    "# Fit the tokenizer on the text and then save the tokenizer.\n",
    "passwordTokenizer = Tokenizer(filters = \"\", lower = True, char_level = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to Save the Tokenizer Configuration\n",
    "def saveTokenizer(TOKENIZER, OUTPUT_PATH):\n",
    "    tokenizerConfigString = TOKENIZER.to_json()\n",
    "    with open(OUTPUT_PATH+\".json\", \"w\") as op_file:\n",
    "        op_file.write(tokenizerConfigString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to Load the Tokenizer Configuration\n",
    "def loadTokenizer(TOKENIZER_FILE_PATH):\n",
    "    _, file_extension = os.path.splitext(TOKENIZER_FILE_PATH)\n",
    "    if(file_extension != \".json\"):\n",
    "        raise Exception(\"Incorrect File.\")\n",
    "        return\n",
    "    else:\n",
    "        with open(TOKENIZER_FILE_PATH, \"r\") as tokenizer_cfg_file:\n",
    "            tokenizer_config = tokenizer_cfg_file.read()\n",
    "            tokenizer_cfg = json.loads(tokenizer_config)\n",
    "            passwordTokenizer = tf.keras.preprocessing.text.tokenizer_from_json(json.dumps(tokenizer_cfg))\n",
    "            return passwordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': None,\n",
       " 'filters': '',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': True,\n",
       " 'oov_token': None,\n",
       " 'document_count': 14105697,\n",
       " 'word_counts': '{\"\\\\t\": 14105697, \"1\": 6729506, \"2\": 5234401, \"3\": 3765169, \"4\": 3389487, \"5\": 3352338, \"6\": 3116090, \"\\\\n\": 14105697, \"7\": 3098762, \"8\": 3565308, \"9\": 3853241, \"p\": 1619704, \"a\": 8828625, \"s\": 4154066, \"w\": 799568, \"o\": 5173138, \"r\": 4576620, \"d\": 2484237, \"i\": 5553157, \"l\": 4460473, \"v\": 1050793, \"e\": 7203479, \"y\": 2373398, \"u\": 2307207, \"n\": 4827931, \"c\": 2608350, \"k\": 2012166, \"b\": 2110821, \"g\": 1717216, \"m\": 3205286, \"j\": 1237461, \"h\": 2335137, \"q\": 178503, \"t\": 3425154, \"0\": 5735322, \"f\": 981496, \"z\": 763424, \"x\": 479316, \"!\": 142923, \";\": 12258, \"-\": 133010, \"*\": 123842, \".\": 248717, \"?\": 18301, \",\": 29750, \"/\": 48190, \"#\": 48873, \"@\": 107880, \"$\": 36029, \"%\": 10254, \"^\": 6394, \"&\": 26359, \"+\": 26989, \"\\'\": 15335, \"[\": 7682, \"]\": 10802, \"<\": 9561, \"_\": 193006, \">\": 2458, \"=\": 18365, \"\\\\\\\\\": 25832, \"\\\\\"\": 3637, \":\": 6858, \"(\": 17037, \")\": 18963, \"`\": 5451, \"~\": 8327, \"{\": 1050, \"}\": 964, \"|\": 700}',\n",
       " 'word_docs': '{\"4\": 2700425, \"6\": 2447173, \"\\\\n\": 14105697, \"3\": 2998313, \"5\": 2601993, \"2\": 4021581, \"\\\\t\": 14105697, \"1\": 5049998, \"7\": 2454981, \"9\": 2888581, \"8\": 2736136, \"a\": 6203639, \"s\": 3435023, \"w\": 746175, \"r\": 3920485, \"d\": 2182950, \"p\": 1391233, \"o\": 3969565, \"y\": 2163861, \"u\": 2070765, \"v\": 993992, \"i\": 4540656, \"l\": 3576048, \"e\": 5325265, \"n\": 3967300, \"c\": 2306277, \"k\": 1806549, \"b\": 1753293, \"g\": 1530627, \"m\": 2763367, \"j\": 1143502, \"h\": 2118370, \"q\": 168747, \"t\": 2885245, \"0\": 3810122, \"f\": 863905, \"z\": 665326, \"x\": 404231, \"!\": 116204, \";\": 10751, \"-\": 107808, \"*\": 80989, \".\": 187176, \"?\": 14422, \",\": 24512, \"/\": 33359, \"#\": 44736, \"^\": 4795, \"%\": 9134, \"$\": 26877, \"@\": 96424, \"&\": 25006, \"+\": 22134, \"\\'\": 14068, \"]\": 9497, \"[\": 6831, \"<\": 8752, \"_\": 170266, \"\\\\\\\\\": 8027, \"=\": 16351, \">\": 1983, \"\\\\\"\": 2352, \":\": 5873, \"(\": 15338, \")\": 16989, \"`\": 4627, \"~\": 5363, \"}\": 878, \"{\": 985, \"|\": 490}',\n",
       " 'index_docs': '{\"18\": 2700425, \"21\": 2447173, \"2\": 14105697, \"15\": 2998313, \"19\": 2601993, \"8\": 4021581, \"1\": 14105697, \"5\": 5049998, \"22\": 2454981, \"14\": 2888581, \"16\": 2736136, \"3\": 6203639, \"13\": 3435023, \"35\": 746175, \"11\": 3920485, \"24\": 2182950, \"31\": 1391233, \"9\": 3969565, \"25\": 2163861, \"27\": 2070765, \"33\": 993992, \"7\": 4540656, \"12\": 3576048, \"4\": 5325265, \"10\": 3967300, \"23\": 2306277, \"29\": 1806549, \"28\": 1753293, \"30\": 1530627, \"20\": 2763367, \"32\": 1143502, \"26\": 2118370, \"40\": 168747, \"17\": 2885245, \"6\": 3810122, \"34\": 863905, \"36\": 665326, \"37\": 404231, \"41\": 116204, \"57\": 10751, \"42\": 107808, \"43\": 80989, \"38\": 187176, \"54\": 14422, \"48\": 24512, \"46\": 33359, \"45\": 44736, \"64\": 4795, \"59\": 9134, \"47\": 26877, \"44\": 96424, \"50\": 25006, \"49\": 22134, \"56\": 14068, \"58\": 9497, \"62\": 6831, \"60\": 8752, \"39\": 170266, \"51\": 8027, \"53\": 16351, \"67\": 1983, \"66\": 2352, \"63\": 5873, \"55\": 15338, \"52\": 16989, \"65\": 4627, \"61\": 5363, \"69\": 878, \"68\": 985, \"70\": 490}',\n",
       " 'index_word': '{\"1\": \"\\\\t\", \"2\": \"\\\\n\", \"3\": \"a\", \"4\": \"e\", \"5\": \"1\", \"6\": \"0\", \"7\": \"i\", \"8\": \"2\", \"9\": \"o\", \"10\": \"n\", \"11\": \"r\", \"12\": \"l\", \"13\": \"s\", \"14\": \"9\", \"15\": \"3\", \"16\": \"8\", \"17\": \"t\", \"18\": \"4\", \"19\": \"5\", \"20\": \"m\", \"21\": \"6\", \"22\": \"7\", \"23\": \"c\", \"24\": \"d\", \"25\": \"y\", \"26\": \"h\", \"27\": \"u\", \"28\": \"b\", \"29\": \"k\", \"30\": \"g\", \"31\": \"p\", \"32\": \"j\", \"33\": \"v\", \"34\": \"f\", \"35\": \"w\", \"36\": \"z\", \"37\": \"x\", \"38\": \".\", \"39\": \"_\", \"40\": \"q\", \"41\": \"!\", \"42\": \"-\", \"43\": \"*\", \"44\": \"@\", \"45\": \"#\", \"46\": \"/\", \"47\": \"$\", \"48\": \",\", \"49\": \"+\", \"50\": \"&\", \"51\": \"\\\\\\\\\", \"52\": \")\", \"53\": \"=\", \"54\": \"?\", \"55\": \"(\", \"56\": \"\\'\", \"57\": \";\", \"58\": \"]\", \"59\": \"%\", \"60\": \"<\", \"61\": \"~\", \"62\": \"[\", \"63\": \":\", \"64\": \"^\", \"65\": \"`\", \"66\": \"\\\\\"\", \"67\": \">\", \"68\": \"{\", \"69\": \"}\", \"70\": \"|\"}',\n",
       " 'word_index': '{\"\\\\t\": 1, \"\\\\n\": 2, \"a\": 3, \"e\": 4, \"1\": 5, \"0\": 6, \"i\": 7, \"2\": 8, \"o\": 9, \"n\": 10, \"r\": 11, \"l\": 12, \"s\": 13, \"9\": 14, \"3\": 15, \"8\": 16, \"t\": 17, \"4\": 18, \"5\": 19, \"m\": 20, \"6\": 21, \"7\": 22, \"c\": 23, \"d\": 24, \"y\": 25, \"h\": 26, \"u\": 27, \"b\": 28, \"k\": 29, \"g\": 30, \"p\": 31, \"j\": 32, \"v\": 33, \"f\": 34, \"w\": 35, \"z\": 36, \"x\": 37, \".\": 38, \"_\": 39, \"q\": 40, \"!\": 41, \"-\": 42, \"*\": 43, \"@\": 44, \"#\": 45, \"/\": 46, \"$\": 47, \",\": 48, \"+\": 49, \"&\": 50, \"\\\\\\\\\": 51, \")\": 52, \"=\": 53, \"?\": 54, \"(\": 55, \"\\'\": 56, \";\": 57, \"]\": 58, \"%\": 59, \"<\": 60, \"~\": 61, \"[\": 62, \":\": 63, \"^\": 64, \"`\": 65, \"\\\\\"\": 66, \">\": 67, \"{\": 68, \"}\": 69, \"|\": 70}'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the Tokenizer on data:\n",
    "passwordTokenizer.fit_on_texts(listPasswords)\n",
    "passwordTokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save Tokenizer -- for later use and load it to avoid re-fitting\n",
    "saveTokenizer(passwordTokenizer, \"prototypeTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': None,\n",
       " 'filters': '',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': True,\n",
       " 'oov_token': None,\n",
       " 'document_count': 14105697,\n",
       " 'word_counts': '{\"\\\\t\": 14105697, \"1\": 6729506, \"2\": 5234401, \"3\": 3765169, \"4\": 3389487, \"5\": 3352338, \"6\": 3116090, \"\\\\n\": 14105697, \"7\": 3098762, \"8\": 3565308, \"9\": 3853241, \"p\": 1619704, \"a\": 8828625, \"s\": 4154066, \"w\": 799568, \"o\": 5173138, \"r\": 4576620, \"d\": 2484237, \"i\": 5553157, \"l\": 4460473, \"v\": 1050793, \"e\": 7203479, \"y\": 2373398, \"u\": 2307207, \"n\": 4827931, \"c\": 2608350, \"k\": 2012166, \"b\": 2110821, \"g\": 1717216, \"m\": 3205286, \"j\": 1237461, \"h\": 2335137, \"q\": 178503, \"t\": 3425154, \"0\": 5735322, \"f\": 981496, \"z\": 763424, \"x\": 479316, \"!\": 142923, \";\": 12258, \"-\": 133010, \"*\": 123842, \".\": 248717, \"?\": 18301, \",\": 29750, \"/\": 48190, \"#\": 48873, \"@\": 107880, \"$\": 36029, \"%\": 10254, \"^\": 6394, \"&\": 26359, \"+\": 26989, \"\\'\": 15335, \"[\": 7682, \"]\": 10802, \"<\": 9561, \"_\": 193006, \">\": 2458, \"=\": 18365, \"\\\\\\\\\": 25832, \"\\\\\"\": 3637, \":\": 6858, \"(\": 17037, \")\": 18963, \"`\": 5451, \"~\": 8327, \"{\": 1050, \"}\": 964, \"|\": 700}',\n",
       " 'word_docs': '{\"4\": 2700425, \"6\": 2447173, \"\\\\n\": 14105697, \"3\": 2998313, \"5\": 2601993, \"2\": 4021581, \"\\\\t\": 14105697, \"1\": 5049998, \"7\": 2454981, \"9\": 2888581, \"8\": 2736136, \"a\": 6203639, \"s\": 3435023, \"w\": 746175, \"r\": 3920485, \"d\": 2182950, \"p\": 1391233, \"o\": 3969565, \"y\": 2163861, \"u\": 2070765, \"v\": 993992, \"i\": 4540656, \"l\": 3576048, \"e\": 5325265, \"n\": 3967300, \"c\": 2306277, \"k\": 1806549, \"b\": 1753293, \"g\": 1530627, \"m\": 2763367, \"j\": 1143502, \"h\": 2118370, \"q\": 168747, \"t\": 2885245, \"0\": 3810122, \"f\": 863905, \"z\": 665326, \"x\": 404231, \"!\": 116204, \";\": 10751, \"-\": 107808, \"*\": 80989, \".\": 187176, \"?\": 14422, \",\": 24512, \"/\": 33359, \"#\": 44736, \"^\": 4795, \"%\": 9134, \"$\": 26877, \"@\": 96424, \"&\": 25006, \"+\": 22134, \"\\'\": 14068, \"]\": 9497, \"[\": 6831, \"<\": 8752, \"_\": 170266, \"\\\\\\\\\": 8027, \"=\": 16351, \">\": 1983, \"\\\\\"\": 2352, \":\": 5873, \"(\": 15338, \")\": 16989, \"`\": 4627, \"~\": 5363, \"}\": 878, \"{\": 985, \"|\": 490}',\n",
       " 'index_docs': '{\"18\": 2700425, \"21\": 2447173, \"2\": 14105697, \"15\": 2998313, \"19\": 2601993, \"8\": 4021581, \"1\": 14105697, \"5\": 5049998, \"22\": 2454981, \"14\": 2888581, \"16\": 2736136, \"3\": 6203639, \"13\": 3435023, \"35\": 746175, \"11\": 3920485, \"24\": 2182950, \"31\": 1391233, \"9\": 3969565, \"25\": 2163861, \"27\": 2070765, \"33\": 993992, \"7\": 4540656, \"12\": 3576048, \"4\": 5325265, \"10\": 3967300, \"23\": 2306277, \"29\": 1806549, \"28\": 1753293, \"30\": 1530627, \"20\": 2763367, \"32\": 1143502, \"26\": 2118370, \"40\": 168747, \"17\": 2885245, \"6\": 3810122, \"34\": 863905, \"36\": 665326, \"37\": 404231, \"41\": 116204, \"57\": 10751, \"42\": 107808, \"43\": 80989, \"38\": 187176, \"54\": 14422, \"48\": 24512, \"46\": 33359, \"45\": 44736, \"64\": 4795, \"59\": 9134, \"47\": 26877, \"44\": 96424, \"50\": 25006, \"49\": 22134, \"56\": 14068, \"58\": 9497, \"62\": 6831, \"60\": 8752, \"39\": 170266, \"51\": 8027, \"53\": 16351, \"67\": 1983, \"66\": 2352, \"63\": 5873, \"55\": 15338, \"52\": 16989, \"65\": 4627, \"61\": 5363, \"69\": 878, \"68\": 985, \"70\": 490}',\n",
       " 'index_word': '{\"1\": \"\\\\t\", \"2\": \"\\\\n\", \"3\": \"a\", \"4\": \"e\", \"5\": \"1\", \"6\": \"0\", \"7\": \"i\", \"8\": \"2\", \"9\": \"o\", \"10\": \"n\", \"11\": \"r\", \"12\": \"l\", \"13\": \"s\", \"14\": \"9\", \"15\": \"3\", \"16\": \"8\", \"17\": \"t\", \"18\": \"4\", \"19\": \"5\", \"20\": \"m\", \"21\": \"6\", \"22\": \"7\", \"23\": \"c\", \"24\": \"d\", \"25\": \"y\", \"26\": \"h\", \"27\": \"u\", \"28\": \"b\", \"29\": \"k\", \"30\": \"g\", \"31\": \"p\", \"32\": \"j\", \"33\": \"v\", \"34\": \"f\", \"35\": \"w\", \"36\": \"z\", \"37\": \"x\", \"38\": \".\", \"39\": \"_\", \"40\": \"q\", \"41\": \"!\", \"42\": \"-\", \"43\": \"*\", \"44\": \"@\", \"45\": \"#\", \"46\": \"/\", \"47\": \"$\", \"48\": \",\", \"49\": \"+\", \"50\": \"&\", \"51\": \"\\\\\\\\\", \"52\": \")\", \"53\": \"=\", \"54\": \"?\", \"55\": \"(\", \"56\": \"\\'\", \"57\": \";\", \"58\": \"]\", \"59\": \"%\", \"60\": \"<\", \"61\": \"~\", \"62\": \"[\", \"63\": \":\", \"64\": \"^\", \"65\": \"`\", \"66\": \"\\\\\"\", \"67\": \">\", \"68\": \"{\", \"69\": \"}\", \"70\": \"|\"}',\n",
       " 'word_index': '{\"\\\\t\": 1, \"\\\\n\": 2, \"a\": 3, \"e\": 4, \"1\": 5, \"0\": 6, \"i\": 7, \"2\": 8, \"o\": 9, \"n\": 10, \"r\": 11, \"l\": 12, \"s\": 13, \"9\": 14, \"3\": 15, \"8\": 16, \"t\": 17, \"4\": 18, \"5\": 19, \"m\": 20, \"6\": 21, \"7\": 22, \"c\": 23, \"d\": 24, \"y\": 25, \"h\": 26, \"u\": 27, \"b\": 28, \"k\": 29, \"g\": 30, \"p\": 31, \"j\": 32, \"v\": 33, \"f\": 34, \"w\": 35, \"z\": 36, \"x\": 37, \".\": 38, \"_\": 39, \"q\": 40, \"!\": 41, \"-\": 42, \"*\": 43, \"@\": 44, \"#\": 45, \"/\": 46, \"$\": 47, \",\": 48, \"+\": 49, \"&\": 50, \"\\\\\\\\\": 51, \")\": 52, \"=\": 53, \"?\": 54, \"(\": 55, \"\\'\": 56, \";\": 57, \"]\": 58, \"%\": 59, \"<\": 60, \"~\": 61, \"[\": 62, \":\": 63, \"^\": 64, \"`\": 65, \"\\\\\"\": 66, \">\": 67, \"{\": 68, \"}\": 69, \"|\": 70}'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passwordTokenizer = loadTokenizer(\"prototypeTokenizer.json\")\n",
    "# Sanity Check\n",
    "passwordTokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLength(FILE_PATH, break_num = 0):\n",
    "    count = 0\n",
    "    with open(FILE_PATH, \"r\") as f:\n",
    "        for count, _ in enumerate(f):\n",
    "            pass\n",
    "    count = count + 1\n",
    "    if(break_num == 0):\n",
    "        return count\n",
    "    else:\n",
    "        return break_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to preprocess and get validation data\n",
    "def getValidationData(VALID_X_FILE_PATH, VALID_Y_FILE_PATH, VOCAB_SIZE, TOKENIZER, MAX_LENGTH, VALIDATION_BATCH_SIZE, TRAIN_ALL = None):\n",
    "    if(TRAIN_ALL is not None):\n",
    "        total_passwords = getLength(VALID_X_FILE_PATH, TRAIN_ALL)\n",
    "    else:\n",
    "        total_passwords = getLength(VALID_X_FILE_PATH)\n",
    "    total_batches = math.floor(total_passwords / VALIDATION_BATCH_SIZE)\n",
    "    total_passwords_to_read = total_batches * VALIDATION_BATCH_SIZE\n",
    "    valid_passwords = []\n",
    "    count = 0\n",
    "    valid_y = []\n",
    "    flag = True\n",
    "    with open(VALID_X_FILE_PATH, \"r\") as valid_file:\n",
    "        for _, password in enumerate(valid_file):\n",
    "            if(_ < total_passwords_to_read):\n",
    "                valid_passwords.append(password.rstrip(\"\\n\"))\n",
    "    with open(VALID_Y_FILE_PATH, \"r\") as valid_true_file:\n",
    "        for _, true_password in enumerate(valid_true_file):\n",
    "            if(_ < total_passwords_to_read):\n",
    "                valid_y.append(true_password)\n",
    "    valid_encoded_passwords = TOKENIZER.texts_to_sequences(valid_passwords)\n",
    "    valid_y_encoded_passwords = TOKENIZER.texts_to_sequences(valid_y)\n",
    "    valid_padded_passwords = pad_sequences(valid_encoded_passwords, padding = \"post\", maxlen = (MAX_LENGTH + 1))\n",
    "    valid_y_padded_passwords = pad_sequences(valid_y_encoded_passwords, padding = \"post\", maxlen = (MAX_LENGTH + 1))\n",
    "    print(len(valid_padded_passwords))\n",
    "    print(len(valid_y_padded_passwords))\n",
    "    for x_valid_password, y_valid_password in zip(valid_padded_passwords, valid_y_padded_passwords):\n",
    "        count += 1\n",
    "        if(count % 500 == 0):\n",
    "            # Give progress feedback\n",
    "            print(f\"Total passwords processed {count}\")\n",
    "        if(flag):\n",
    "            final_x_valid = np.array(x_valid_password).reshape(1, (MAX_LENGTH + 1))\n",
    "            temp_y_valid = np.array(y_valid_password).reshape(1, (MAX_LENGTH + 1))\n",
    "            final_y_valid = to_categorical(temp_y_valid, num_classes = (VOCAB_SIZE + 1))\n",
    "            flag = False\n",
    "        else:\n",
    "            final_x_valid = np.concatenate((final_x_valid, np.array(x_valid_password).reshape(1, (MAX_LENGTH + 1))), axis = 0)\n",
    "            temp_y_valid = np.array(y_valid_password).reshape(1, (MAX_LENGTH + 1))                                 \n",
    "            final_y_valid = np.concatenate((final_y_valid, to_categorical(temp_y_valid, num_classes = (VOCAB_SIZE + 1))), axis = 0)\n",
    "    return(final_x_valid, final_y_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an input pipeline for feeding encoded passwords -- \n",
    "# Pass train_passwords as None if you want to train on entire set of passwords mentioned in password_file\n",
    "class gruNetworkInputSequence(Sequence):\n",
    "    def __init__(self, train_passwords, password_file_x, password_file_y, batch_size, tokenizer, max_length, vocab_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.train_passwords = train_passwords\n",
    "        self.password_file = password_file_x\n",
    "        self.password_file_y = password_file_y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        with open(self.password_file, \"r\") as pass_file:\n",
    "            for count, _ in enumerate(pass_file):\n",
    "                pass\n",
    "        total_passwords = count + 1\n",
    "        if(self.train_passwords is not None):\n",
    "            if(self.train_passwords < total_passwords):\n",
    "                return math.floor(self.train_passwords / self.batch_size)\n",
    "            else:\n",
    "                return math.floor(total_passwords / self.batch_size)\n",
    "        else:\n",
    "            return math.floor(total_passwords / self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # This could be slow, will try to improve speed later by saving encoded passwords.\n",
    "        # batch_enc_padded_passwords = None\n",
    "        # batch_y_true = None\n",
    "        flag = True\n",
    "        batch_enc_padded_passwords = []\n",
    "        batch_y_true = []\n",
    "        # temp_x = []\n",
    "        # temp_y = []\n",
    "        batch_passwords = []\n",
    "        batch_y_passwords = []\n",
    "        batch_password_index = list(range((index) * self.batch_size, ((index + 1) * self.batch_size)))\n",
    "        with open(self.password_file, \"r\") as pass_file:\n",
    "            for count, password in enumerate(pass_file):\n",
    "                if(count in batch_password_index):\n",
    "                    batch_passwords.append(password.rstrip(\"\\n\"))\n",
    "                else:\n",
    "                    continue\n",
    "        with open(self.password_file_y, \"r\") as pass_file_y:\n",
    "            for count_y, password_y in enumerate(pass_file_y):\n",
    "                if(count_y in batch_password_index):\n",
    "                    batch_y_passwords.append(password_y)\n",
    "                else:\n",
    "                    continue\n",
    "        #print(f\"{batch_passwords}\\n\\n\\n{batch_y_passwords}\")\n",
    "        encoded_passwords = self.tokenizer.texts_to_sequences(batch_passwords)\n",
    "        padded_encoded_passwords = pad_sequences(encoded_passwords, padding = \"post\", maxlen = (self.max_length + 1))\n",
    "        encoded_y_passwords = self.tokenizer.texts_to_sequences(batch_y_passwords)\n",
    "        padded_encoded_y_passwords = pad_sequences(encoded_y_passwords, padding = \"post\", maxlen = (self.max_length + 1))\n",
    "        for encoded_password, encoded_y_password in zip(padded_encoded_passwords, padded_encoded_y_passwords):\n",
    "            reshaped_example = np.array(encoded_password).reshape(1, (self.max_length + 1))\n",
    "            reshaped_y_example = np.array(encoded_y_password).reshape(1, (self.max_length + 1))\n",
    "            if(flag):\n",
    "                batch_enc_padded_passwords = reshaped_example\n",
    "                batch_y_true = to_categorical(y = reshaped_y_example, num_classes = (self.vocab_size + 1))\n",
    "                flag = False\n",
    "            else:\n",
    "                batch_enc_padded_passwords = np.concatenate((batch_enc_padded_passwords, reshaped_example), axis = 0)\n",
    "                batch_y_true = np.concatenate((batch_y_true, to_categorical(y = reshaped_y_example,  num_classes = (self.vocab_size + 1))), axis = 0)\n",
    "        return (np.array(batch_enc_padded_passwords).astype(np.float32), np.array(batch_y_true).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTFDataInputPipeline(X_INPUT_FILE_PATH, Y_INPUT_FILE_PATH, TOKENIZER, MAX_LENGTH, BUFFER_SIZE, PREFETCH_BATCHES, BATCH_SIZE):\n",
    "    x_inputs = []\n",
    "    y_inputs = []\n",
    "    with open(X_INPUT_FILE_PATH, \"r\") as x_pass_file:\n",
    "        with open(Y_INPUT_FILE_PATH, \"r\") as y_pass_file:\n",
    "            for count, x_y_pass in enumerate(zip(x_pass_file, y_pass_file)):\n",
    "                x_pass = x_y_pass[0].rstrip(\"\\n\")\n",
    "                y_pass = x_y_pass[1]\n",
    "                x_inputs.append(x_pass)\n",
    "                y_inputs.append(y_pass)\n",
    "    \n",
    "    # All passwords read in memory:\n",
    "    x_tokenized = TOKENIZER.texts_to_sequences(x_inputs)\n",
    "    y_tokenized = TOKENIZER.texts_to_sequences(y_inputs)\n",
    "    \n",
    "    x_padded = pad_sequences(x_tokenized, padding = \"post\", maxlen = (MAX_LENGTH + 1))\n",
    "    y_padded = pad_sequences(y_tokenized, padding = \"post\", maxlen = (MAX_LENGTH + 1))\n",
    "    \n",
    "    x_np_tokenized = np.array(x_padded)\n",
    "    y_np_tokenized = np.array(y_padded)\n",
    "    \n",
    "    x_dataset = tf.data.Dataset.from_tensor_slices(x_np_tokenized)\n",
    "    y_dataset = tf.data.Dataset.from_tensor_slices(y_np_tokenized)\n",
    "    \n",
    "    fit_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\n",
    "    fit_dataset = fit_dataset.shuffle(buffer_size = BUFFER_SIZE)\n",
    "    fit_dataset = fit_dataset.cache()\n",
    "    fit_dataset = fit_dataset.batch(BATCH_SIZE)\n",
    "    fit_dataset = fit_dataset.prefetch(PREFETCH_BATCHES)\n",
    "    \n",
    "    return fit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Preparing the Data to splitting into train and validation\n",
    "# 2. Then prepare the train Data to split again into train and test\n",
    "# 3. Don't train on test \n",
    "# 4. Modify hyperparameters on validate\n",
    "# 5. Plot using TensorBoard for both train and validation for each epoch \n",
    "# 6. Necessary to train if for atleast one epoch, to plot the graphs\n",
    "# Or we can choose to plot for each batch - but will be very\n",
    "# Resource intensive and will slow us down considerably.\n",
    "# 7. train_test_split is given the input and output arrays as X & Y\n",
    "X_train_test_train, X_validation, Y_train_test_train, Y_validation = train_test_split(inputPasswords, outPasswords, test_size = 0.05, shuffle = True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_test_train, Y_train_test_train, test_size = 0.1, shuffle = True)\n",
    "\n",
    "# Sanity Check\n",
    "print(f\"{repr(X_train_test_train[100])}\\t{repr(Y_train_test_train[100])}\")\n",
    "print(f\"{repr(X_validation[100])}\\t{repr(Y_validation[100])}\")\n",
    "print(f\"{repr(X_train[100])}\\t{repr(Y_train[100])}\")\n",
    "print(f\"{repr(X_test[100])}\\t{repr(Y_test[100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing to save models - dedicated methods for it\n",
    "# We also save them after each epoch using checkpoint callback.\n",
    "# So saving manually is optional.\n",
    "def saveKerasModel(MODEL, OUTPUT_MODEL_PATH):\n",
    "    # Saves the model to the disk, saves both the architecture and \n",
    "    # the configuration.\n",
    "    try:\n",
    "        assert isinstance(MODEL, Model)\n",
    "        MODEL.save(OUTPUT_MODEL_PATH)\n",
    "        print(f\"[+] Model has been successfully saved to {OUTPUT_MODEL_PATH}\")\n",
    "    except:\n",
    "        raise Exception(\"Model instance is incorrect. Failed!\")\n",
    "        return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to load the model manually.\n",
    "# Is Extremely Important!\n",
    "def loadKerasModel(INPUT_MODEL_PATH):\n",
    "    loaded_model = tf.keras.models.load_model(INPUT_MODEL_PATH)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘train’: File exists\n",
      "mkdir: cannot create directory ‘test’: File exists\n",
      "mkdir: cannot create directory ‘validation’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir train\n",
    "!mkdir test\n",
    "!mkdir validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to write the output to file -- \n",
    "def writeOutput(FILE_PATH, FILE_TYPE_TRAIN, PASS_LIST_X, PASS_LIST_Y):\n",
    "    with open(os.path.join(FILE_PATH, FILE_TYPE_TRAIN + \"_X\" + \".txt\"), \"w\") as x_file:\n",
    "        for input_password in PASS_LIST_X:\n",
    "            x_file.write(input_password + \"\\n\")\n",
    "    with open(os.path.join(FILE_PATH, FILE_TYPE_TRAIN + \"_Y\" + \".txt\"), \"w\") as y_file:\n",
    "        for out_password in PASS_LIST_Y:\n",
    "            y_file.write(out_password)\n",
    "    print(\"[+] Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the split data into their respective directories --\n",
    "writeOutput(\".\\\\train\", \"train\", X_train, Y_train)\n",
    "writeOutput(\".\\\\test\", \"test\", X_test, Y_test)\n",
    "writeOutput(\".\\\\validation\", \"validation\", X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tellatubar\n",
      "\ttupapiruloybofo\n",
      "ellatubar\n",
      "tupapiruloybofo\n",
      "\tc.u2033\n",
      "\tgoldsun414\n",
      "c.u2033\n",
      "goldsun414\n",
      "\t242518\n",
      "\t1001338\n",
      "242518\n",
      "1001338\n",
      "12060370 ./train/train_X.txt\n",
      "12060370 ./train/train_Y.txt\n",
      "1340042 ./test/test_X.txt\n",
      "1340042 ./test/test_Y.txt\n",
      "705285 ./validation/validation_X.txt\n",
      "705285 ./validation/validation_Y.txt\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "!head -n 2 ./train/train_X.txt \n",
    "!head -n 2 ./train/train_Y.txt\n",
    "!head -n 2 ./test/test_X.txt \n",
    "!head -n 2 ./test/test_Y.txt \n",
    "!head -n 2 ./validation/validation_X.txt \n",
    "!head -n 2 ./validation/validation_Y.txt \n",
    "!wc -l ./train/train_X.txt \n",
    "!wc -l ./train/train_Y.txt \n",
    "!wc -l ./test/test_X.txt\n",
    "!wc -l ./test/test_Y.txt\n",
    "!wc -l ./validation/validation_X.txt \n",
    "!wc -l ./validation/validation_Y.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\n",
      "mkdir: cannot create directory ‘tensorboard_log_dir’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Preparing the file directory structure for callbacks:\n",
    "!mkdir checkpoints\n",
    "!mkdir tensorboard_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating Optimizer Instance for Training Model\n",
    "adam_optimizer = Adam(learning_rate = LEARNING_RATE, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building teacher-forcing training model\n",
    "# Must include shared architecture for following layers --\n",
    "# 1. Embedding Layer\n",
    "# 2. LSTM_1 \n",
    "# 3. LSTM_2\n",
    "# 4. LSTM_3\n",
    "# 5. Dense_1\n",
    "# 6. Dense_2 (Softmax - Outputs Probability Distribution over |VOCAB_SIZE + <PAD_TOKEN>|)\n",
    "\n",
    "# The shared layers are defined below -- \n",
    "shared_embedding_layer = Embedding(input_dim = (VOCAB_SIZE + 1), output_dim = EMBEDDING_DIMENSION, mask_zero = True, name = \"Embedding_Layer\")\n",
    "shared_lstm_1 = LSTM(LSTM_UNITS, return_sequences = True, return_state = True)\n",
    "shared_lstm_2 = LSTM(LSTM_UNITS, return_sequences = True, return_state = True)\n",
    "shared_lstm_3 = LSTM(LSTM_UNITS, return_sequences = True, return_state = True)\n",
    "shared_dense_1 = TimeDistributed(Dense(DENSE_UNITS, activation = \"relu\"))\n",
    "shared_dense_op = TimeDistributed(Dense((VOCAB_SIZE + 1), activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 33)]              0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 33, 16)            1136      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                [(None, 33, 300), (None,  380400    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               [(None, 33, 300), (None,  721200    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               [(None, 33, 300), (None,  721200    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 33, 100)           30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 33, 71)            7171      \n",
      "=================================================================\n",
      "Total params: 1,861,207\n",
      "Trainable params: 1,861,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1. Defining the Teacher-Force Training Model:\n",
    "# 2. Do not use initial_state call argument for the LSTM layers\n",
    "# 3. Use the initial_state call arguemnt for 'all' the LSTM layers \n",
    "# in the Inference Model\n",
    "\n",
    "train_input = Input(shape = ((MAX_LENGTH + 1),))\n",
    "train_emb_op = shared_embedding_layer(train_input)\n",
    "train_lstm_1_op, train_lstm_1_hidden, train_lstm_1_cell = shared_lstm_1(train_emb_op)\n",
    "train_lstm_2_op, train_lstm_2_hidden, train_lstm_2_cell = shared_lstm_2(train_lstm_1_op)\n",
    "train_lstm_3_op, train_lstm_3_hidden, train_lstm_3_cell = shared_lstm_3(train_lstm_2_op)\n",
    "train_dense_1_op = shared_dense_1(train_lstm_3_op)\n",
    "train_model_op = shared_dense_op(train_dense_1_op)\n",
    "\n",
    "train_model = Model(inputs = train_input, outputs = train_model_op)\n",
    "\n",
    "# Check the model summary \n",
    "print(train_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compiling the Model with specified callbacks and metrics:\n",
    "train_model.compile(optimizer = adam_optimizer, loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics = METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Layer (Embedding)     multiple             1136        input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   multiple             380400      Embedding_Layer[1][0]            \n",
      "                                                                 input_18[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  multiple             721200      lstm_9[1][0]                     \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  multiple             721200      lstm_10[1][0]                    \n",
      "                                                                 input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,823,936\n",
      "Trainable params: 1,823,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the Inference Model\n",
    "# Needs the initial_state call argument for all the LSTM layers\n",
    "\n",
    "inference_input = Input(shape = (1,))\n",
    "lstm_1_hidden = Input(shape = (LSTM_UNITS,))\n",
    "lstm_1_cell = Input(shape = (LSTM_UNITS,))\n",
    "lstm_2_hidden = Input(shape = (LSTM_UNITS,))\n",
    "lstm_2_cell = Input(shape = (LSTM_UNITS,))\n",
    "lstm_3_hidden = Input(shape = (LSTM_UNITS,))\n",
    "lstm_3_cell = Input(shape = (LSTM_UNITS,))\n",
    "inference_emb_op = shared_embedding_layer(inference_input)\n",
    "inference_lstm_1_op, inference_lstm_1_hidden, inference_lstm_1_cell = shared_lstm_1(inference_emb_op, initial_state = [lstm_1_hidden, lstm_1_cell])\n",
    "inference_lstm_2_op, inference_lstm_2_hidden, inference_lstm_2_cell = shared_lstm_2(inference_lstm_1_op, initial_state = [lstm_2_hidden, lstm_2_cell])\n",
    "inference_lstm_3_op, inference_lstm_3_hidden, inference_lstm_3_cell = shared_lstm_3(inference_lstm_2_op, initial_state = [lstm_3_hidden, lstm_3_cell])\n",
    "inference_dense_1_op = shared_dense_1(inference_lstm_3_op)\n",
    "inference_model_op = shared_dense_op(inference_dense_1_op)\n",
    "\n",
    "inputs_list = [inference_input, lstm_1_hidden, lstm_1_cell, lstm_2_hidden, lstm_2_cell, lstm_3_hidden, lstm_3_cell]\n",
    "outputs_list = [inference_lstm_1_hidden, inference_lstm_1_cell, inference_lstm_2_hidden, inference_lstm_2_cell, inference_lstm_3_hidden, inference_lstm_3_cell]\n",
    "\n",
    "inference_model = Model(inputs = inputs_list, outputs = outputs_list)\n",
    "\n",
    "print(inference_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.compile(optimizer = \"adam\", loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics = METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a metadata file for tensorboard embedding visualization\n",
    "def createEmbeddingMetadata(TOKENIZER):\n",
    "    tokenized_chars = [token for token, idx in TOKENIZER.word_index.items()]\n",
    "    with open(EMBEDDING_METADATA, 'w') as emb_file:\n",
    "        emb_file.write(\"<PAD>\\n\")\n",
    "        for token in tokenized_chars:\n",
    "            if (token == '\\n'):\n",
    "                emb_file.write(\"\\\\n\\n\")\n",
    "                continue\n",
    "            if(token == '\\t'):\n",
    "                emb_file.write(\"\\\\t\\n\")\n",
    "                continue\n",
    "            emb_file.write(token+\"\\n\")\n",
    "\n",
    "createEmbeddingMetadata(passwordTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring all model callbacks\n",
    "# 1. LearningRate Callback\n",
    "# 2. ModelCheckpoint Callback\n",
    "# 3. TensorBoard Callback\n",
    "# 4. GradientClipping Callback\n",
    "# Gradient Clipping is required because LSTMs/GRUs greatly suffer from both\n",
    "# Exploding and Vanishing Gradients which presents as big problem and sets\n",
    "# all values of delta-wt, delta-b to NaN causing numerical instability.\n",
    "# This occurs because of Gradient-Descent through Time or as we call it\n",
    "# Back-Propogation through time algorithm causing the gradients to be multiplied \n",
    "# with <1 or >1 weight values across all timesteps leaving the network extremely\n",
    "# vulnerable to numerical instability.\n",
    "\n",
    "# LearningRateScheduler - Using Exponential Learning Rate Decay\n",
    "# First defining a 'schedule' for learning rate decay\n",
    "def expLearningRateDecay(epoch):\n",
    "   initial_lrate = 0.003\n",
    "   k = 0.1\n",
    "   lrate = initial_lrate * math.exp(-k * epoch)\n",
    "   return lrate\n",
    "\n",
    "# Scheduler Callback:\n",
    "train_learning_rate_callback = LearningRateScheduler(schedule = expLearningRateDecay, verbose = 1)\n",
    "\n",
    "# Checkpoint Callback:\n",
    "train_checkpoint_callback = ModelCheckpoint(\".\\\\checkpoints\\\\Checkpoint-{epoch:03d}\", verbose = 1, save_weights_only = False, save_freq = \"epoch\")\n",
    "\n",
    "# TensorBoard Callback:\n",
    "train_tensorboard_callback = TensorBoard(log_dir = \".\\\\tensorboard_log_dir\", histogram_freq = 1, write_graph = True, write_images = True, update_freq = 'batch', embeddings_freq = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Custom Callback\n",
    "# Tf stable version 2.x currently has a bug\n",
    "# with saved_model format of saving models.\n",
    "# Loading of saved_model formatted serialized\n",
    "# model requires tf-nightly-gpu version which \n",
    "# contains latest bug fixes.\n",
    "# We require a custom callback that saves the \n",
    "# entire model in h5 format which can be loaded\n",
    "# without any problem.\n",
    "\n",
    "# We will also implement to save inference_model \n",
    "# at another directory everytime. \n",
    "\n",
    "# This custom callback will work with the newer\n",
    "# saved_model format, thus allowing flexibility.\n",
    "\n",
    "class saveH5CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epochs, logs = None):\n",
    "        # Saving the train model in h5 format. This should always be defined.\n",
    "        self.model.save(filepath = \".\\\\h5_train_checkpoints\\\\H5-Checkpoint-{epoch:03d}.h5\".format(epoch = (epochs + 1)), include_optimizer = True, save_format = \"h5\")\n",
    "        try:\n",
    "            # Check if the inference_model is defined.\n",
    "            # If defined save the model in h5 format\n",
    "            global inference_model\n",
    "            inference_model.save(filepath = \".\\\\h5_inference_checkpoints\\\\H5-Inference-Checkpoint-{epoch:03d}.h5\".format(epoch = (epochs + 1)), include_optimizer = True, save_format = \"h5\")\n",
    "        except:\n",
    "            # If Inference Model not defined, log the message.\n",
    "            with open(\"inference_log.txt\", \"a\") as inf_err:\n",
    "                inf_err.write(\"Inference Model not defined for epoch {epoch:03d}.\\nSaving train model only. Build Inference Model Later.\\n\\n\".format(epoch = (epochs + 1)))\n",
    "\n",
    "        \n",
    "# Creating an instance of custom callback.    \n",
    "saveH5Callback = saveH5CustomCallback()\n",
    "\n",
    "# Compiling all callback together..\n",
    "callbacks = [train_learning_rate_callback, train_checkpoint_callback, train_tensorboard_callback, saveH5Callback]\n",
    "\n",
    "# GradientClipping Callback\n",
    "\n",
    "# GradientClipping has to be done manually by computing the gradient wrt \n",
    "# the loss of each trainable parameter and then clip it.\n",
    "# After which the gradients have to be applied to the weights / biases / gates\n",
    "# thus preventing exploding/vanishing gradient.\n",
    "\n",
    "# This will only be done if the model experiences any sort of numerical \n",
    "# instability and throws NaN Exceptions since this process has capacity \n",
    "# to cripple model's speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Sequence Object Created!\n",
      "1984\n",
      "1984\n",
      "Total passwords processed 500\n",
      "Total passwords processed 1000\n",
      "Total passwords processed 1500\n",
      "[+] Validation Data is read into volatile memory!\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data into volatile memory so we \n",
    "# don't have to re-perform all the calculation during each \n",
    "# validation test run.\n",
    "# Fitting the model might be very resource intensive\n",
    "# Due to data fetch from physical disk and prior pre-processing.\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "# WARNING:\n",
    "# Sequence class objects as input to the models suffer from\n",
    "# extensive deadlocks. Coupling this with model.fit arg \n",
    "# use_multiprocessing = True can failed memory allocation\n",
    "# process during fitting. \n",
    "# For flexibility, the sequence objects have been implemented.\n",
    "# However, their use is not recommended as they are unstable \n",
    "# and slow down the pre-fetching capabilities.\n",
    "# tf.data.Dataset implementations are preferred and are defined \n",
    "# above. Please use those to implement input and output pipelines\n",
    "# for both the training and validation sets. \n",
    "\n",
    "####################################################################\n",
    "\n",
    "# DO NOT EXECUTE.\n",
    "input_pipeline = gruNetworkInputSequence(10000, \".\\\\train\\\\train_X.txt\", \".\\\\train\\\\train_Y.txt\", BATCH_SIZE, passwordTokenizer, MAX_LENGTH, VOCAB_SIZE)\n",
    "print(\"[+] Sequence Object Created!\")\n",
    "validation_data = getValidationData(\".\\\\validation\\\\validation_X.txt\", \".\\\\validation\\\\validation_Y.txt\", VOCAB_SIZE, passwordTokenizer, MAX_LENGTH, BATCH_SIZE, 2000)\n",
    "print(\"[+] Validation Data is read into volatile memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip_pipeline = createTFDataInputPipeline(\".\\\\train\\\\train_X.txt\", \".\\\\train\\\\train_Y.txt\", passwordTokenizer, MAX_LENGTH, SHUFFLE_BUFFER_SIZE, PREFETCH_BATCHES, BATCH_SIZE)\n",
    "validation_ip_pipeline = createTFDataInputPipeline(\".\\\\validation\\\\validation_X.txt\", \".\\\\validation\\\\validation_Y.txt\", passwordTokenizer, MAX_LENGTH, SHUFFLE_BUFFER_SIZE, PREFETCH_BATCHES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 376887 steps, validate for 22041 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 1/15\n",
      "     1/376887 [..............................] - ETA: 1181:51:47 - loss: 1.1827 - sparse_categorical_crossentropy: 4.2627WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.104082). Check your callbacks.\n",
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.7237 - sparse_categorical_crossentropy: 2.5182\n",
      "Epoch 00001: saving model to .\\checkpoints\\Checkpoint-001\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-001\\assets\n",
      "376887/376887 [==============================] - 18700s 50ms/step - loss: 0.7237 - sparse_categorical_crossentropy: 2.5182 - val_loss: 0.7138 - val_sparse_categorical_crossentropy: 2.4841\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009048374180359595.\n",
      "Epoch 2/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.115026). Check your callbacks.\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.7112 - sparse_categorical_crossentropy: 2.4746\n",
      "Epoch 00002: saving model to .\\checkpoints\\Checkpoint-002\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-002\\assets\n",
      "376887/376887 [==============================] - 17371s 46ms/step - loss: 0.7112 - sparse_categorical_crossentropy: 2.4746 - val_loss: 0.7101 - val_sparse_categorical_crossentropy: 2.4711\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0008187307530779819.\n",
      "Epoch 3/15\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.7082 - sparse_categorical_crossentropy: 2.4640\n",
      "Epoch 00003: saving model to .\\checkpoints\\Checkpoint-003\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-003\\assets\n",
      "376887/376887 [==============================] - 18407s 49ms/step - loss: 0.7082 - sparse_categorical_crossentropy: 2.4640 - val_loss: 0.7081 - val_sparse_categorical_crossentropy: 2.4641\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007408182206817179.\n",
      "Epoch 4/15\n",
      "111932/376887 [=======>......................] - ETA: 3:24:32 - loss: 0.7067 - sparse_categorical_crossentropy: 2.4588WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.104842). Check your callbacks.\n",
      "111934/376887 [=======>......................] - ETA: 3:24:32 - loss: 0.7067 - sparse_categorical_crossentropy: 2.4588WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.104842). Check your callbacks.\n",
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.7062 - sparse_categorical_crossentropy: 2.4570\n",
      "Epoch 00004: saving model to .\\checkpoints\\Checkpoint-004\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-004\\assets\n",
      "376887/376887 [==============================] - 18380s 49ms/step - loss: 0.7062 - sparse_categorical_crossentropy: 2.4570 - val_loss: 0.7065 - val_sparse_categorical_crossentropy: 2.4585\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0006703200460356394.\n",
      "Epoch 5/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.112025). Check your callbacks.\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.7045 - sparse_categorical_crossentropy: 2.4514\n",
      "Epoch 00005: saving model to .\\checkpoints\\Checkpoint-005\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-005\\assets\n",
      "376887/376887 [==============================] - 18719s 50ms/step - loss: 0.7045 - sparse_categorical_crossentropy: 2.4514 - val_loss: 0.7052 - val_sparse_categorical_crossentropy: 2.4541\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0006065306597126335.\n",
      "Epoch 6/15\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.7032 - sparse_categorical_crossentropy: 2.4466- ETA: 3s - loss: 0.703\n",
      "Epoch 00006: saving model to .\\checkpoints\\Checkpoint-006\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-006\\assets\n",
      "376887/376887 [==============================] - 18487s 49ms/step - loss: 0.7032 - sparse_categorical_crossentropy: 2.4466 - val_loss: 0.7043 - val_sparse_categorical_crossentropy: 2.4509\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005488116360940264.\n",
      "Epoch 7/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.108024). Check your callbacks.\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.7020 - sparse_categorical_crossentropy: 2.4425\n",
      "Epoch 00007: saving model to .\\checkpoints\\Checkpoint-007\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-007\\assets\n",
      "376887/376887 [==============================] - 18678s 50ms/step - loss: 0.7020 - sparse_categorical_crossentropy: 2.4425 - val_loss: 0.7034 - val_sparse_categorical_crossentropy: 2.4478\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004965853037914095.\n",
      "Epoch 8/15\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.7009 - sparse_categorical_crossentropy: 2.4387\n",
      "Epoch 00008: saving model to .\\checkpoints\\Checkpoint-008\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-008\\assets\n",
      "376887/376887 [==============================] - 19329s 51ms/step - loss: 0.7009 - sparse_categorical_crossentropy: 2.4387 - val_loss: 0.7027 - val_sparse_categorical_crossentropy: 2.4453\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004493289641172216.\n",
      "Epoch 9/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.109025). Check your callbacks.\n",
      "119561/376887 [========>.....................] - ETA: 3:27:12 - loss: 0.7002 - sparse_categorical_crossentropy: 2.4364WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.106358). Check your callbacks.\n",
      "119562/376887 [========>.....................] - ETA: 3:27:12 - loss: 0.7002 - sparse_categorical_crossentropy: 2.4364WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.106358). Check your callbacks.\n",
      "119563/376887 [========>.....................] - ETA: 3:27:12 - loss: 0.7002 - sparse_categorical_crossentropy: 2.4364WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.106358). Check your callbacks.\n",
      "119564/376887 [========>.....................] - ETA: 3:27:13 - loss: 0.7002 - sparse_categorical_crossentropy: 2.4364WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.106358). Check your callbacks.\n",
      "119565/376887 [========>.....................] - ETA: 3:27:13 - loss: 0.7002 - sparse_categorical_crossentropy: 2.4364WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.106358). Check your callbacks.\n",
      "119566/376887 [========>.....................] - ETA: 3:27:13 - loss: 0.7002 - sparse_categorical_crossentropy: 2.4364WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.104397). Check your callbacks.\n",
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.6999 - sparse_categorical_crossentropy: 2.4354\n",
      "Epoch 00009: saving model to .\\checkpoints\\Checkpoint-009\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-009\\assets\n",
      "376887/376887 [==============================] - 18255s 48ms/step - loss: 0.6999 - sparse_categorical_crossentropy: 2.4354 - val_loss: 0.7021 - val_sparse_categorical_crossentropy: 2.4432\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00040656965974059914.\n",
      "Epoch 10/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.112025). Check your callbacks.\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.6991 - sparse_categorical_crossentropy: 2.4323\n",
      "Epoch 00010: saving model to .\\checkpoints\\Checkpoint-010\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-010\\assets\n",
      "376887/376887 [==============================] - 17910s 48ms/step - loss: 0.6991 - sparse_categorical_crossentropy: 2.4323 - val_loss: 0.7016 - val_sparse_categorical_crossentropy: 2.4415\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00036787944117144236.\n",
      "Epoch 11/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116026). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.6983 - sparse_categorical_crossentropy: 2.4295\n",
      "Epoch 00011: saving model to .\\checkpoints\\Checkpoint-011\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-011\\assets\n",
      "376887/376887 [==============================] - 18585s 49ms/step - loss: 0.6983 - sparse_categorical_crossentropy: 2.4295 - val_loss: 0.7011 - val_sparse_categorical_crossentropy: 2.4397\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00033287108369807955.\n",
      "Epoch 12/15\n",
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.6975 - sparse_categorical_crossentropy: 2.4269\n",
      "Epoch 00012: saving model to .\\checkpoints\\Checkpoint-012\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-012\\assets\n",
      "376887/376887 [==============================] - 18439s 49ms/step - loss: 0.6975 - sparse_categorical_crossentropy: 2.4269 - val_loss: 0.7007 - val_sparse_categorical_crossentropy: 2.4383\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00030119421191220205.\n",
      "Epoch 13/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116027). Check your callbacks.\n",
      "376885/376887 [============================>.] - ETA: 0s - loss: 0.6968 - sparse_categorical_crossentropy: 2.4245\n",
      "Epoch 00013: saving model to .\\checkpoints\\Checkpoint-013\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-013\\assets\n",
      "376887/376887 [==============================] - 19265s 51ms/step - loss: 0.6968 - sparse_categorical_crossentropy: 2.4245 - val_loss: 0.7004 - val_sparse_categorical_crossentropy: 2.4374\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002725317930340126.\n",
      "Epoch 14/15\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.113025). Check your callbacks.\n",
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.6962 - sparse_categorical_crossentropy: 2.4225\n",
      "Epoch 00014: saving model to .\\checkpoints\\Checkpoint-014\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-014\\assets\n",
      "376887/376887 [==============================] - 18181s 48ms/step - loss: 0.6962 - sparse_categorical_crossentropy: 2.4225 - val_loss: 0.7002 - val_sparse_categorical_crossentropy: 2.4366\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00024659696394160646.\n",
      "Epoch 15/15\n",
      " 82254/376887 [=====>........................] - ETA: 4:05:54 - loss: 0.6961 - sparse_categorical_crossentropy: 2.4218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116941). Check your callbacks.\n",
      " 82255/376887 [=====>........................] - ETA: 4:05:54 - loss: 0.6961 - sparse_categorical_crossentropy: 2.4218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116941). Check your callbacks.\n",
      " 82256/376887 [=====>........................] - ETA: 4:05:54 - loss: 0.6961 - sparse_categorical_crossentropy: 2.4218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116941). Check your callbacks.\n",
      " 82257/376887 [=====>........................] - ETA: 4:05:54 - loss: 0.6961 - sparse_categorical_crossentropy: 2.4218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116941). Check your callbacks.\n",
      "376886/376887 [============================>.] - ETA: 0s - loss: 0.6957 - sparse_categorical_crossentropy: 2.4205\n",
      "Epoch 00015: saving model to .\\checkpoints\\Checkpoint-015\n",
      "INFO:tensorflow:Assets written to: .\\checkpoints\\Checkpoint-015\\assets\n",
      "376887/376887 [==============================] - 19445s 52ms/step - loss: 0.6957 - sparse_categorical_crossentropy: 2.4205 - val_loss: 0.6999 - val_sparse_categorical_crossentropy: 2.4356\n"
     ]
    }
   ],
   "source": [
    "train_model_history = train_model.fit(x = train_ip_pipeline, epochs = EPOCHS, callbacks = callbacks, verbose = VERBOSITY, validation_data = validation_ip_pipeline, validation_freq = 1)\n",
    "# Callbacks and validation will be done every epoch.\n",
    "# To change this, \n",
    "# change definitions for callbacks &,\n",
    "# change the validation_freq \n",
    "# arg in model.fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM 3.7",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
